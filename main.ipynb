{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "from rasterio.transform import from_origin\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd \n",
    "import pandas as pd\n",
    "from dnb_annual import *\n",
    "from variables import years, composites, region_map, region_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this script is used only once to generate the regional images for each year\n",
    "# country_polygons = gpd.read_file(\"geoBoundaries-UKR-ADM1.geojson\")\n",
    "\n",
    "# for year in years:\n",
    "#     dnb = dnb_annual(year, composites, country_polygons)\n",
    "#     dnb.load_all_data()\n",
    "#     dnb.save_rasters()\n",
    "#     dnb.load_rasters()\n",
    "#     dnb.build_regional_images()\n",
    "#     dnb.add_padding()\n",
    "#     dnb.save_regional_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this script is used to clean gdp data\n",
    "\n",
    "# Inflation data\n",
    "# inflation = pd.read_excel(\"data/isc_reg.xls\", skiprows=2, header=1)\n",
    "# inflation = inflation.drop(columns=inflation.columns[0])\n",
    "# inflation = inflation.rename(columns={inflation.columns[-1]: \"region\"})\n",
    "# inflation = inflation[~inflation[\"region\"].isin([\"Ukraine\", \"oblasts\"])]\n",
    "# inflation = inflation.dropna()\n",
    "# inflation[\"region\"] = inflation[\"region\"].map(region_map)\n",
    "# inflation.columns = inflation.columns.astype(str)\n",
    "# inflation = inflation.melt(id_vars=\"region\", var_name=\"year\", value_name=\"inflation\")\n",
    "# inflation.to_csv(\"data/inflation.csv\", index=False)\n",
    "\n",
    "# GDP data\n",
    "gdp = pd.read_excel(\"data/ukr_reg_gdp.xls\", skiprows=3, header=1)\n",
    "gdp = gdp.drop(columns=gdp.columns[0])\n",
    "gdp = gdp.iloc[:, np.r_[18:36, -1]]\n",
    "gdp = gdp.rename(columns={gdp.columns[-1]: \"region\"})\n",
    "gdp = gdp[~gdp[\"region\"].isin([\"Ukrane\", \"oblasts\"])]\n",
    "gdp = gdp.dropna()\n",
    "gdp[\"region\"] = gdp[\"region\"].map(region_map)\n",
    "gdp[\"region\"] = gdp[\"region\"].fillna(\"Sevastopol\")\n",
    "gdp.columns = gdp.columns.astype(str)\n",
    "gdp = gdp.rename(columns={gdp.columns[i]: gdp.columns[i][:4] for i in range(18)})\n",
    "gdp = gdp.melt(id_vars=\"region\", var_name=\"year\", value_name=\"real_gdp_change\")\n",
    "\n",
    "# include only years from 2012 inclusive, exclude Sevastopol and the Autonomous Republic of Crimea\n",
    "gdp = gdp[gdp[\"year\"].astype(int) >= 2012]\n",
    "gdp = gdp[~gdp[\"region\"].isin([\"Sevastopol\", \"Autonomous Republic of Crimea\"])]\n",
    "\n",
    "# set the value for the starting year to 100 (2012), NaN for the rest\n",
    "gdp.loc[gdp[\"year\"] == \"2012\", \"real_gdp\"] = 100\n",
    "gdp = gdp.sort_values(by=[\"region\", \"year\"])\n",
    "gdp[\"real_gdp_change\"] = gdp[\"real_gdp_change\"] / 100\n",
    "\n",
    "# reste the index\n",
    "gdp = gdp.reset_index(drop=True)\n",
    "\n",
    "# # calculate the real gdp\n",
    "for i in range(1, gdp.shape[0]):\n",
    "\n",
    "    # skip if the year is 2012\n",
    "    if gdp.loc[gdp.index[i], \"year\"] == \"2012\":\n",
    "        continue\n",
    "    else:\n",
    "        gdp.loc[gdp.index[i], \"real_gdp\"] = gdp.loc[gdp.index[i-1], \"real_gdp\"] * (gdp.loc[gdp.index[i], \"real_gdp_change\"])\n",
    "\n",
    "# delete the real_gdp_change column\n",
    "gdp = gdp.drop(columns=\"real_gdp_change\")\n",
    "\n",
    "# get the nominal gdp\n",
    "gdp_nominal = pd.read_excel(\"data/ukr_reg_gdp.xls\", skiprows=3, header=1)\n",
    "gdp_nominal = gdp_nominal.iloc[:, np.r_[9, -1]]\n",
    "gdp_nominal.columns = [\"gdp_nominal\", \"region\"]\n",
    "gdp_nominal = gdp_nominal[~gdp_nominal[\"region\"].isin([\"Ukrane\", \"oblasts\"])]\n",
    "gdp_nominal = gdp_nominal.dropna()\n",
    "gdp_nominal[\"region\"] = gdp_nominal[\"region\"].map(region_map)\n",
    "gdp_nominal[\"region\"] = gdp_nominal[\"region\"].fillna(\"Sevastopol\")\n",
    "\n",
    "# merge nominal gdp to real gdp by region\n",
    "gdp = gdp.merge(gdp_nominal, on=\"region\")\n",
    "\n",
    "# multiple the real gdp by the nominal gdp\n",
    "gdp[\"real_gdp\"] = gdp[\"real_gdp\"] * gdp[\"gdp_nominal\"]\n",
    "\n",
    "# drop the nominal gdp column\n",
    "gdp = gdp.drop(columns=\"gdp_nominal\")\n",
    "\n",
    "# for the region column, change all spaces to _\n",
    "gdp[\"region\"] = gdp[\"region\"].str.replace(\" \", \"_\")\n",
    "\n",
    "# save the data\n",
    "gdp.to_csv(\"data/clean_gdp.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# # Loading the MNIST dataset\n",
    "# from keras.datasets import mnist\n",
    "# (train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 765, 1076)\n",
      "(250,)\n",
      "2.842170943040401e-17\n",
      "53922.0\n"
     ]
    }
   ],
   "source": [
    "# load clean gdp data\n",
    "gdp = pd.read_csv(\"data/clean_gdp.csv\")\n",
    "\n",
    "# Initialise a three dimensional array to store the images\n",
    "X = np.zeros((len(gdp), 765, 1076))\n",
    "y = np.zeros(len(gdp))\n",
    "\n",
    "# load the snow covered and snow free images, add them together and append to the list\n",
    "for i in range(len(gdp)):\n",
    "\n",
    "    # get year, region, and gdp\n",
    "    year = gdp[\"year\"][i]\n",
    "    region = gdp[\"region\"][i]\n",
    "    gdp_value = gdp[\"real_gdp\"][i]\n",
    "\n",
    "    # get the file name\n",
    "    file_name = f\"{year}_{region}.h5\"\n",
    "\n",
    "    # load the image\n",
    "    file_path = f\"data/annual_region_images/{file_name}\"\n",
    "    \n",
    "    with h5py.File(file_path, 'r') as annual_region:\n",
    "        snow_covered = annual_region[\"AllAngle_Composite_Snow_Covered\"][:]\n",
    "        snow_free = annual_region[\"AllAngle_Composite_Snow_Free\"][:]\n",
    "\n",
    "        # add the two images together\n",
    "        combined = snow_covered + snow_free\n",
    "\n",
    "    # add the gdp value to y\n",
    "    y[i] = gdp_value\n",
    "\n",
    "    # append the image to X\n",
    "    X[i] = combined\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# Normalise the images\n",
    "maximum = X.max()\n",
    "X = X / maximum\n",
    "\n",
    "# standardise gdp values\n",
    "y = (y - y.mean()) / y.std()\n",
    "\n",
    "print(y.mean())\n",
    "print(maximum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select 80% of the data for training, choose randomly\n",
    "# X is the images, y is the gdp\n",
    "train_size = int(0.8 * len(gdp))\n",
    "test_size = len(gdp) - train_size\n",
    "\n",
    "# select randomly train_size numbers from 0 to len(gdp)\n",
    "train_indices = np.random.choice(len(gdp), train_size, replace=False)\n",
    "test_indices = np.setdiff1d(np.arange(len(gdp)), train_indices)\n",
    "\n",
    "# get the train data\n",
    "X_train = X[train_indices]\n",
    "y_train = y[train_indices]\n",
    "\n",
    "# get the test data\n",
    "X_test = X[test_indices]\n",
    "y_test = y[test_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential() \n",
    "model.add(Flatten(input_shape=(765, 1076)))  # Flatten the 2D image into a 1D array\n",
    "model.add(Dense(64, activation='relu'))      # Add a fully connected layer with 64 neurons and ReLU activation\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer=Adam(), loss='mean_squared_error', metrics=['mae'])  # Using Mean Squared Error loss and Mean Absolute Error metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5/5 [==============================] - 8s 1s/step - loss: 0.8142 - mae: 0.5239 - val_loss: 0.7031 - val_mae: 0.4430\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 3s 671ms/step - loss: 0.5185 - mae: 0.3712 - val_loss: 0.4030 - val_mae: 0.2897\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 3s 723ms/step - loss: 0.3393 - mae: 0.2907 - val_loss: 0.2124 - val_mae: 0.2263\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 3s 697ms/step - loss: 0.1962 - mae: 0.2245 - val_loss: 0.0885 - val_mae: 0.1593\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 3s 659ms/step - loss: 0.1077 - mae: 0.1664 - val_loss: 0.0316 - val_mae: 0.1336\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 4s 765ms/step - loss: 0.0642 - mae: 0.1351 - val_loss: 0.0225 - val_mae: 0.1099\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 4s 742ms/step - loss: 0.0434 - mae: 0.1176 - val_loss: 0.0284 - val_mae: 0.1165\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 4s 733ms/step - loss: 0.0463 - mae: 0.1098 - val_loss: 0.0417 - val_mae: 0.1256\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 4s 725ms/step - loss: 0.0469 - mae: 0.1003 - val_loss: 0.0477 - val_mae: 0.1287\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 3s 688ms/step - loss: 0.0427 - mae: 0.0877 - val_loss: 0.0372 - val_mae: 0.1141\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x20a8006eca0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)  # Assuming you have a validation split of 20%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0382 - mae: 0.1212\n",
      "Test MAE: 0.1212017610669136\n",
      "Test Loss: 0.038211364299058914\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Evaluate your model on the testing data\n",
    "test_loss, test_mae = model.evaluate(X_test, y_test)\n",
    "print('Test MAE:', test_mae) # mean absolute error\n",
    "print('Test Loss:', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.35328459, -0.35934255, -0.35308317, -0.67422427, -0.67006618,\n",
       "       -0.65813516, -0.66242736,  1.66620088,  1.36667144,  1.34279448,\n",
       "        0.21447579, -0.40856682, -0.37869668, -0.31488347,  0.45270047,\n",
       "        0.40657239, -0.5671497 , -0.55565071, -0.44506385, -0.42874331,\n",
       "       -0.48154707, -0.50134819, -0.44644959, -0.46444809,  4.02686291,\n",
       "        4.22617341,  0.23412913,  0.2694953 , -0.32585884, -0.6289909 ,\n",
       "        0.17647786, -0.40386827, -0.32935416, -0.33432836,  0.27269501,\n",
       "        0.31760439, -0.07728294, -0.10081219, -0.53458293, -0.53685923,\n",
       "       -0.50489344, -0.46038454, -0.4519557 , -0.60057928, -0.576305  ,\n",
       "       -0.59695028, -0.27076813, -0.56921405,  0.04834337, -0.46934308])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
