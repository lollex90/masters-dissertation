{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jakub\\anaconda\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "c:\\Users\\jakub\\anaconda\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "from rasterio.transform import from_origin\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd \n",
    "import pandas as pd\n",
    "from dnb_annual import *\n",
    "from variables import years, composites, region_map, region_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this script is used only once to generate the regional images for each year\n",
    "# country_polygons = gpd.read_file(\"geoBoundaries-UKR-ADM1.geojson\")\n",
    "\n",
    "# for year in years:\n",
    "#     dnb = dnb_annual(year, composites, country_polygons)\n",
    "#     dnb.load_all_data()\n",
    "#     dnb.save_rasters()\n",
    "#     dnb.load_rasters()\n",
    "#     dnb.build_regional_images()\n",
    "#     dnb.add_padding()\n",
    "#     dnb.save_regional_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this script is used to clean gdp data\n",
    "\n",
    "# Inflation data\n",
    "# inflation = pd.read_excel(\"data/isc_reg.xls\", skiprows=2, header=1)\n",
    "# inflation = inflation.drop(columns=inflation.columns[0])\n",
    "# inflation = inflation.rename(columns={inflation.columns[-1]: \"region\"})\n",
    "# inflation = inflation[~inflation[\"region\"].isin([\"Ukraine\", \"oblasts\"])]\n",
    "# inflation = inflation.dropna()\n",
    "# inflation[\"region\"] = inflation[\"region\"].map(region_map)\n",
    "# inflation.columns = inflation.columns.astype(str)\n",
    "# inflation = inflation.melt(id_vars=\"region\", var_name=\"year\", value_name=\"inflation\")\n",
    "# inflation.to_csv(\"data/inflation.csv\", index=False)\n",
    "\n",
    "# GDP data\n",
    "gdp = pd.read_excel(\"data/ukr_reg_gdp.xls\", skiprows=3, header=1)\n",
    "gdp = gdp.drop(columns=gdp.columns[0])\n",
    "gdp = gdp.iloc[:, np.r_[18:36, -1]]\n",
    "gdp = gdp.rename(columns={gdp.columns[-1]: \"region\"})\n",
    "gdp = gdp[~gdp[\"region\"].isin([\"Ukrane\", \"oblasts\"])]\n",
    "gdp = gdp.dropna()\n",
    "gdp[\"region\"] = gdp[\"region\"].map(region_map)\n",
    "gdp[\"region\"] = gdp[\"region\"].fillna(\"Sevastopol\")\n",
    "gdp.columns = gdp.columns.astype(str)\n",
    "gdp = gdp.rename(columns={gdp.columns[i]: gdp.columns[i][:4] for i in range(18)})\n",
    "gdp = gdp.melt(id_vars=\"region\", var_name=\"year\", value_name=\"real_gdp_change\")\n",
    "\n",
    "# include only years from 2012 inclusive, exclude Sevastopol and the Autonomous Republic of Crimea\n",
    "gdp = gdp[gdp[\"year\"].astype(int) >= 2012]\n",
    "gdp = gdp[~gdp[\"region\"].isin([\"Sevastopol\", \"Autonomous Republic of Crimea\"])]\n",
    "\n",
    "# set the value for the starting year to 100 (2012), NaN for the rest\n",
    "gdp.loc[gdp[\"year\"] == \"2012\", \"real_gdp\"] = 100\n",
    "gdp = gdp.sort_values(by=[\"region\", \"year\"])\n",
    "gdp[\"real_gdp_change\"] = gdp[\"real_gdp_change\"] / 100\n",
    "\n",
    "# reste the index\n",
    "gdp = gdp.reset_index(drop=True)\n",
    "\n",
    "# # calculate the real gdp\n",
    "for i in range(1, gdp.shape[0]):\n",
    "\n",
    "    # skip if the year is 2012\n",
    "    if gdp.loc[gdp.index[i], \"year\"] == \"2012\":\n",
    "        continue\n",
    "    else:\n",
    "        gdp.loc[gdp.index[i], \"real_gdp\"] = gdp.loc[gdp.index[i-1], \"real_gdp\"] * (gdp.loc[gdp.index[i], \"real_gdp_change\"])\n",
    "\n",
    "# delete the real_gdp_change column\n",
    "gdp = gdp.drop(columns=\"real_gdp_change\")\n",
    "\n",
    "# get the nominal gdp\n",
    "gdp_nominal = pd.read_excel(\"data/ukr_reg_gdp.xls\", skiprows=3, header=1)\n",
    "gdp_nominal = gdp_nominal.iloc[:, np.r_[9, -1]]\n",
    "gdp_nominal.columns = [\"gdp_nominal\", \"region\"]\n",
    "gdp_nominal = gdp_nominal[~gdp_nominal[\"region\"].isin([\"Ukrane\", \"oblasts\"])]\n",
    "gdp_nominal = gdp_nominal.dropna()\n",
    "gdp_nominal[\"region\"] = gdp_nominal[\"region\"].map(region_map)\n",
    "gdp_nominal[\"region\"] = gdp_nominal[\"region\"].fillna(\"Sevastopol\")\n",
    "\n",
    "# merge nominal gdp to real gdp by region\n",
    "gdp = gdp.merge(gdp_nominal, on=\"region\")\n",
    "\n",
    "# multiple the real gdp by the nominal gdp\n",
    "gdp[\"real_gdp\"] = gdp[\"real_gdp\"] * gdp[\"gdp_nominal\"]\n",
    "\n",
    "# drop the nominal gdp column\n",
    "gdp = gdp.drop(columns=\"gdp_nominal\")\n",
    "\n",
    "# for the region column, change all spaces to _\n",
    "gdp[\"region\"] = gdp[\"region\"].str.replace(\" \", \"_\")\n",
    "\n",
    "# save the data\n",
    "gdp.to_csv(\"data/clean_gdp.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jakub\\anaconda\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jakub\\anaconda\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Resizing, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# # Loading the MNIST dataset\n",
    "# from keras.datasets import mnist\n",
    "# (train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load clean gdp data\n",
    "gdp = pd.read_csv(\"data/clean_gdp.csv\")\n",
    "\n",
    "# Initialise a three dimensional array to store the images with the shape (number of images, height, width, channels)\n",
    "X = np.zeros((len(gdp), 765, 1076, 4))\n",
    "y = np.zeros(len(gdp))\n",
    "\n",
    "# load the snow covered and snow free images, add them together and append to the list\n",
    "for i in range(len(gdp)):\n",
    "\n",
    "    # get year, region, and gdp\n",
    "    year = gdp[\"year\"][i]\n",
    "    region = gdp[\"region\"][i]\n",
    "    gdp_value = gdp[\"real_gdp\"][i]\n",
    "\n",
    "    # get the file name\n",
    "    file_name = f\"{year}_{region}.h5\"\n",
    "\n",
    "    # load the image\n",
    "    file_path = f\"data/annual_region_images/{file_name}\"\n",
    "    \n",
    "    with h5py.File(file_path, 'r') as annual_region:\n",
    "        nearnad_snow_cov = annual_region[\"NearNadir_Composite_Snow_Covered\"][:]\n",
    "        nearnad_snow_free = annual_region[\"NearNadir_Composite_Snow_Free\"][:]\n",
    "        offnad_snow_cov = annual_region[\"OffNadir_Composite_Snow_Covered\"][:]\n",
    "        offnad_snow_free = annual_region[\"OffNadir_Composite_Snow_Free\"][:]\n",
    "\n",
    "        # add the two images together\n",
    "        # combined = snow_covered + snow_free\n",
    "\n",
    "    # add the gdp value to y\n",
    "    y[i] = gdp_value\n",
    "\n",
    "    # append both images as two channels to to X\n",
    "    X[i, :, :, 0] = nearnad_snow_cov\n",
    "    X[i, :, :, 1] = nearnad_snow_free\n",
    "    X[i, :, :, 2] = offnad_snow_cov\n",
    "    X[i, :, :, 3] = offnad_snow_free\n",
    "\n",
    "# print(X.shape)\n",
    "# print(y.shape)\n",
    "\n",
    "# Normalise the images\n",
    "maximum = X.max()\n",
    "X = X / maximum\n",
    "\n",
    "# standardise gdp values\n",
    "y_mean = y.mean()\n",
    "y_std = y.std()\n",
    "y = (y - y_mean) / y_std\n",
    "\n",
    "# print(y.mean())\n",
    "# print(maximum)\n",
    "\n",
    "# change the format to a float16\n",
    "X = X.astype(np.float16)\n",
    "y = y.astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select 80% of the data for training, choose randomly\n",
    "# X is the images, y is the gdp\n",
    "train_size = int(0.8 * len(gdp))\n",
    "test_size = len(gdp) - train_size\n",
    "\n",
    "# select randomly train_size numbers from 0 to len(gdp)\n",
    "train_indices = np.random.choice(len(gdp), train_size, replace=False)\n",
    "test_indices = np.setdiff1d(np.arange(len(gdp)), train_indices)\n",
    "\n",
    "# get the train data\n",
    "X_train = X[train_indices]\n",
    "y_train = y[train_indices]\n",
    "\n",
    "# get the test data\n",
    "X_test = X[test_indices]\n",
    "y_test = y[test_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jakub\\anaconda\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\jakub\\anaconda\\lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# Resizing the images\n",
    "model.add(Resizing(300, 440, input_shape=(765, 1076, 4)))\n",
    "# Start with Convolutional layers\n",
    "model.add(Conv2D(16, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))  # Additional Conv layer\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))  # Additional Conv layer\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "# Flatten the results to feed into a dense layer\n",
    "model.add(Flatten())\n",
    "# Add dense layers (hidden layers)\n",
    "model.add(Dense(128, activation='relu'))  # Upscaled dense layer\n",
    "model.add(Dense(64, activation='relu'))   # Additional dense layer\n",
    "# Output layer\n",
    "model.add(Dense(1))\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='mean_squared_error', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# Resizing the images\n",
    "model.add(Resizing(300, 440, input_shape=(765, 1076, 4)))\n",
    "\n",
    "# Start with Convolutional layers\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))  # Increased number of filters\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))  # Increased number of filters\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))  # Increased number of filters\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), activation='relu'))  # Increased number of filters\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# Flatten the results to feed into a dense layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add dense layers (hidden layers)\n",
    "model.add(Dense(256, activation='relu'))  # Increased the number of neurons\n",
    "model.add(Dense(128, activation='relu'))  # Increased the number of neurons\n",
    "model.add(Dense(64, activation='relu'))   # Kept as is for detailed feature extraction\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='mean_squared_error', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 765, 1076, 4)\n",
      "0.896\n"
     ]
    }
   ],
   "source": [
    "# check the size of X_train and y_train\n",
    "print(X_train.shape)\n",
    "print(X_train[1, :, :, 0].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.96 GiB for an array with shape (160, 765, 1076, 4) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20016/2668918783.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Assuming you have a validation split of 20%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\jakub\\anaconda\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jakub\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    101\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m   \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.96 GiB for an array with shape (160, 765, 1076, 4) and data type float32"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)  # Assuming you have a validation split of 20%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 110ms/step - loss: 0.0166 - mae: 0.0931\n",
      "Test MAE: 0.09311747550964355\n",
      "Test Loss: 0.016602778807282448\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Evaluate your model on the testing data\n",
    "test_loss, test_mae = model.evaluate(X_test, y_test)\n",
    "print('Test MAE:', test_mae) # mean absolute error\n",
    "print('Test Loss:', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 120ms/step\n",
      "[-0.3865  -0.3867  -0.3267   1.54     1.303    1.925    0.2595  -0.3655\n",
      " -0.4136  -0.3245   0.3606  -0.555   -0.5557  -0.5557  -0.445   -0.4287\n",
      " -0.514   -0.4731   0.3145   0.2273   0.2201   0.12396 -0.6284   0.267\n",
      "  0.3418  -0.376   -0.381   -0.3293  -0.38     0.3176   0.0857   0.03027\n",
      " -0.06903 -0.523   -0.549   -0.5312  -0.4678  -0.585   -0.2986  -0.2491\n",
      " -0.542   -0.5083  -0.569   -0.542    0.05505  0.04834  0.02493  0.03052\n",
      "  0.05487 -0.4001 ]\n",
      "[-0.37542757 -0.3699261  -0.26612532  1.1491622   1.2016343   1.848887\n",
      "  0.53231966 -0.5138074  -0.49566388 -0.37560466  0.47047287 -0.5861057\n",
      " -0.5505346  -0.58583593 -0.44584957 -0.48678458 -0.5754301  -0.49369138\n",
      "  0.21880376  0.29775324  0.2946698  -0.30644888 -0.616561    0.08748098\n",
      "  0.2635282  -0.4152276  -0.45284614 -0.4336341  -0.45927814  0.28671312\n",
      " -0.15302671 -0.23077467 -0.24244285 -0.5394714  -0.56930745 -0.56435776\n",
      " -0.5435089  -0.54009134 -0.354613   -0.325707   -0.5915505  -0.59282357\n",
      " -0.5294734  -0.5876064  -0.04870503 -0.07072468 -0.04373667 -0.0505455\n",
      " -0.10456258 -0.5146471 ]\n",
      "[ 2.85841227e-02  4.34234142e-02  1.85314417e-01  2.53809750e-01\n",
      "  7.76060820e-02  3.94417644e-02 -1.05115843e+00 -4.05848503e-01\n",
      " -1.98488355e-01 -1.57619834e-01 -3.04710150e-01 -5.57119846e-02\n",
      "  9.23120975e-03 -5.42987585e-02 -1.75523758e-03 -1.35461092e-01\n",
      " -1.19165063e-01 -4.34261560e-02  3.04176867e-01 -3.09986353e-01\n",
      " -3.38843584e-01  3.47211146e+00  1.88679695e-02  6.72466099e-01\n",
      "  2.28991807e-01 -1.04397535e-01 -1.88249707e-01 -3.16653252e-01\n",
      " -2.08999515e-01  9.73274708e-02  2.78574753e+00  8.62300873e+00\n",
      " -2.51209855e+00 -3.15941572e-02 -3.73146534e-02 -6.23204708e-02\n",
      " -1.61906242e-01  7.67052770e-02 -1.87649131e-01 -3.07296276e-01\n",
      " -9.14373398e-02 -1.66284919e-01  6.92175627e-02 -8.41602087e-02\n",
      "  1.88468194e+00  2.46307230e+00  2.75417757e+00  2.65627480e+00\n",
      "  2.90562105e+00 -2.86146760e-01]\n",
      "0.75145686\n"
     ]
    }
   ],
   "source": [
    "y_hat = model.predict(X_test).flatten()\n",
    "\n",
    "print(y_test)\n",
    "print(y_hat)\n",
    "\n",
    "print(1 - y_hat/y_test)\n",
    "\n",
    "print(np.mean(np.abs(1 - y_hat/y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 56ms/step\n",
      "0.10076753842329474\n",
      "36.946041487261475\n"
     ]
    }
   ],
   "source": [
    "# get the predictions from X_test\n",
    "y_hat = model.predict(X_test).flatten()\n",
    "\n",
    "# convert the predictions back to the original scale\n",
    "# y_hat = y_hat * y_std + y_mean\n",
    "# y_test = y_test * y_std + y_mean\n",
    "\n",
    "# compute the mae\n",
    "mae = np.mean(np.abs(y_test - y_hat))\n",
    "print(mae)\n",
    "\n",
    "# compute the mean percentage error\n",
    "percentage_error = np.mean(100*np.abs((y_test - y_hat) / y_test))\n",
    "print(percentage_error)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
